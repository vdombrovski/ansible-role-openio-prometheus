---
groups:
  - name: default
    rules:
      - alert: UnreachableNode
        expr: probe_success{module='icmpcheck'} == 0
        for: 5m
        labels:
          severity: medium
        annotations:
          summary: Unreachable node
          description: >-
            The server '{{ $labels.host }}' can't be reached by the admin machine.
          solutions: >-
            First check that you can reach the node by other means (ssh). Your firewall configuration may also prevent
            the server from being reachable.
            If your host has been rebooted,
            make sure that all services are running by using <code>gridinit_cmd status</code>.
            Host downtime may cause services to be given a score of 0.
            In this case, please refer to the score alert procedures.
            The alert will be resolved once the node is joinable via ICMP.

      - alert: LowMemory
        expr: 9 * sum(netdata_system_ram_MB_average{dimension!~'used'}) by (host) <
          sum(netdata_system_ram_MB_average{dimension='used'}) by (host)
        for: 5m
        labels:
          severity: low
        annotations:
          summary: Low on memory
          description: >-
            RAM usage on server '{{ $labels.host }}' is approaching critical levels.
          solutions: >-
            Check for processes using too much memory on the machine. Refer to the 'OpenIO System' graph in Grafana to
            see memory usage per type of process.
            If an external process uses too much memory, try to identify it,
            as it might have been caused by a memory leak.
            If the process is an OpenIO service (mostly meta2), contact the support team.
            The alert will be resolved once the used RAM is below 90%.

      - alert: LowDiskSpace
        expr: 9 * sum(netdata_disk_space_GB_average{dimension='avail'}) by (host, family) <
          sum(netdata_disk_space_GB_average{dimension='used'}) by (host, family)
        for: 10s
        labels:
          severity: low
          volume: "{{ $labels.family }}"
        annotations:
          summary: Low disk space
          description: >-
            The partition mounted on '{{ $labels.family }}' on server '{{ $labels.host }}' is almost full.
          solutions: >-
            If the partition is used by the system, check for processes that might use space,
            such as old kernels, or logs.
            As a reminder, all OpenIO service logs are logrotated.
            When the partition is used by an OpenIO service, it won't fill up more that 95%.
            Prolongated service downtime may cause the storage on the platform to become unbalanced.
            Please contact the OpenIO support team for more advice on this issue.
            The alert will be resolved once the disk space is freed below 90% use.

      - alert: ServerErrors
        expr: netdata_web_log_response_codes_requests_persec_average{dimension='5xx'} > 0
        for: 10m
        labels:
          severity: medium
        annotations:
          summary: Server errors
          description: >-
            The service '{{ $labels.service }}' on server '{{ $labels.host }}' has had some errors.
          solutions: >-
            Check logfiles associated with the service for 5xx errors,
            and contact the OpenIO support for further assistance.
            The alert will be resolved once there are no more 5xx errors
            in OpenIO logs for the {{ $labels.service }} service.

      - alert: BuriedEvents
        expr: sum(netdata_beanstalk_jobs_jobs_average{dimension='buried'}) by (host) > 0
        for: 10m
        labels:
          severity: low
        annotations:
          summary: Background jobs failures
          description: >-
            Some background jobs on server '{{ $labels.host }}' resulted in errors and couldn't be processed.
          solutions: >-
            Try re-kicking the events by running <code>openio events exhume --oio-ns OPENIO</code> on each node.
            You can check how many events are buried by running <code>openio events stats --oio-ns OPENIO</code>.
            If the problem persists, contact the OpenIO support team.
            The alert will be resolved once there are no more buried events on the node.

      - alert: MetaDown
        expr: probe_success{service_type=~'meta0|meta1|meta2'} == 0
        for: 30s
        labels:
          severity: medium
        annotations:
          summary: Metadata service down
          description: >-
            The service '{{ $labels.service }}' on server '{{ $labels.host }}' doesn't respond to healthchecks.
          solutions: >-
            Collect status by running <code>gridinit_cmd status @{{$labels.service}}</code>
            on the node {{$labels.host}}.
            If the service is down, try repairing it <code>gridinit_cmd repair</code>.
            If the service is still down, check for I/O errors and other causes preventing the service from starting.
            Also look into the logs located in <code>/var/log/oio/sds/</code> for that particular instance.
            If the problem perists, contact the OpenIO support.
            The alert will be resolved once all '{{$labels.service}}' instances respond OK
            to <code>oiotool ping IP:PORT</code> command.

      - alert: RawxDown
        expr: probe_success{service_type='rawx'} == 0
        for: 30s
        labels:
          severity: low
        annotations:
          summary: RAWX service down
          description: >-
            The service '{{ $labels.service }}' on server '{{ $labels.host }}' doesn't respond to healthchecks.
            This could indicate underlying data disk failure, which needs to be replaced
          solutions: >-
            Start by fetching the status of all services on the cluster
            <code>openio cluster list rawx | grep ' False '</code>.
            Check the associated volumes for I/O errors. Replace the disks if I/O errors have been detected,
            then try repairing by using <code>gridinit_cmd repair @rawx</code> on node '{{ $labels.host }}'.
            Unlock the score of the service <code>openio cluster unlock rawx '{{ $labels.service }}'</code>,
            then follow the procedure to start rebuilding data on the volume.
            If you are not able to determine the issue, please contact the OpenIO support.
            The alert will be resolved once the rawx instance '{{ $labels.service }}'
            is marked as up in `openio cluster list` and is scored.

      - alert: RedisDown
        expr: probe_success{service_type=~'redis|redissentinel'} == 0
        for: 60s
        labels:
          severity: medium
        annotations:
          summary: Redis backend malfunction
          description: >-
            The service '{{ $labels.service }}' on server '{{ $labels.host }}' doesn't respond to healthchecks.
            This can cause server errors and must be investigated.
          solutions: >-
            Start by checking that the instance is up by running <code>gridinit_cmd status</code>
            on '{{ $labels.host }}'.
            If the instance is broken, consult the logs of the service located in /var/log/oio/sds/.
            Check that the underlying storage device is functional.
            If the problem persists, please contact the OpenIO support.
            The alert will be resolved once all redis/redissentinel services are up and running.

      - alert: RedisNoMaster
        expr: sum(label_replace(netdata_redis_master_master_average, 'cluster', '$1', 'family', '.*:(\\d+)'))
          by (cluster) < 1
        for: 30s
        labels:
          severity: medium
          cluster: "{{ $labels.cluster }}"
        annotations:
          summary: No master found for redis cluster '{{ $labels.cluster }}'
          description: >-
            One of the redis clusters doesn't have a master. This may cause a complete S3 service failure,
            and needs to be addressed.
          solutions: >-
            Check that all redis services are running by issuing <code>gridinit_cmd status2 @redis</code>
            Refer to the procedure called 'Repair and diagnose account service' in your operations handbook
            If the problem persists, please contact OpenIO support
            The issue will be resolved once a master has been established on the redis cluster.

      - alert: RedisSplitBrain
        expr: sum(sum(label_replace(netdata_redis_replicas_count_average, 'cluster', '$1', 'family', '.*:(\\d+)'))
          by (family, cluster) *
          sum(label_replace(netdata_redis_master_master_average, 'cluster', '$1', 'family', '.*:(\\d+)'))
          by (family, cluster)) by (cluster) !=
          (count(label_replace(netdata_redis_replicas_count_average, 'cluster', '$1', 'family', '.*:(\\d+)'))
          by (cluster) - 1)
        for: 30s
        labels:
          severity: high
          cluster: "{{ $labels.cluster }}"
        annotations:
          summary: Redis split brain on '{{ $labels.cluster }}'
          description: >-
            An unexpected redis cluster replica count. This could indicate a split-brain situation,
            and needs to be addressed immediately
          solutions: >-
            Identify the redis instances part of the malfunctioning cluster.
            You can graph the following request in Grafana:
              <code>netdata_redis_replicas_count_average{family=~'.*:{{ $labels.cluster }}'}</code>
            Stop all redis instances of the cluster by running <code>gridinit_cmd stop OPENIO-redis-(id)</code>
            on them to prevent further split-brain damage. Contact OpenIO support for further steps.

      - alert: ServiceDown
        expr: probe_success{service_type!~'meta\\d|redis.*|rawx|'} == 0
        for: 60s
        labels:
          severity: medium
        annotations:
          summary: Service {{ $labels.service_type }} down
          description: >-
            The service '{{ $labels.service }}' on server '{{ $labels.host }}' doesn't respond to healthchecks.

      - alert: AccountMalfunction
        expr: (count(probe_success{service_type='account'} == 1) == 0)
          or sum(netdata_openio_score__average{service='account'}) == 0
        for: 30s
        labels:
          severity: medium
        annotations:
          summary: Account service malfunction
          description: >-
            The account service is experiencing issues that could cause SLA degradation and server errors.
          solutions: >-
            Start by checking the status of all account services by running <code>openio cluster list account</code>.
            If you have instances marked as 'False',
            try repairing them by running <code>gridinit_cmd repair @account</code>
            on the corresponding nodes.
            Consult the logs at /var/log/oio/sds/ for errors. If you have instances scored at 0,
            run <code>openio cluster unlockall account</code> and check again to make sure that the score is rising.
            If the issue persists, please contact the OpenIO support.
            The alert will be resolved once there is at least one account instance running and scored on the cluster.

      - alert: IAM Backend malfunction
        expr: count(probe_success{service_type=~'galera|keystone'} == 0) > 0
        for: 30s
        labels:
          severity: medium
        annotations:
          summary: IAM Backend malfunction
          description: >-
            The identity management service is malfunctioning. This could eventually result in permission denied errors
            on the gateway.
          solutions: >-
            Start by checking the status of the galera mysql service by running <code>systemctl status mariadb</code>
            Individual services can be restarted using <code>galera_recovery</code> and <code>galera_new_cluster</code>
            commands.
            If the backend is working, proceed to checking keystone logs and the service by issuing
            <code>gridinit_cmd status @keystone</code>.
            When no errors can be found, please contact the OpenIO support. If a package update, or a node configuration
            change has been made prior to this alert, please include its details in the ticket.
            The alert will be resolved once all instances of the IAM service are responding correctly.

      - alert: MetricCollectionFailure
        expr: up{job=~'blackbox|netdata'} == 0
        for: 60s
        labels:
          severity: medium
          service: "{{ $labels.module }}"
        annotations:
          summary: Metric collection failure
          description: >-
            The metric collection service '{{ $labels.module }}' on server '{{ $labels.host }}' is down.
            This prevents monitoring from working properly and must be resolved.
          solutions: >-
            Check the state of the service by running
            <code>systemctl status {{ if eq $labels.module "blackbox" }}blackbox_exporter{{ else }}netdata{{ end }}.
            If the service isn't started, start and enable it.
            If the service fails to start, contact the OpenIO support.
            Check that the service is reachable, if not, make sure ports 19999 and 9115 are open.
            You can test the availability by doing a HTTP GET request on the IP/port of the service.
            The alert will be resolved when '{{ $labels.module }}' is up and running
            and can be contacted by the admin machine.
...
